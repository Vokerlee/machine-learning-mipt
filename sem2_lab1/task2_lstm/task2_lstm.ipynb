{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip experiment.zip -d .\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./experiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment string below to download dataset\n",
    "# !wget https://storage.yandexcloud.net/natasha-nerus/data/nerus_lenta.conllu.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nerus import load_nerus\n",
    "docs = load_nerus('nerus_lenta.conllu.gz')\n",
    "special_tokens = ['<UNK>', '<PAD>']\n",
    "special_tags = ['<PAD>']\n",
    "\n",
    "sentences = []\n",
    "tags = []\n",
    "\n",
    "cnt = 0\n",
    "n_docs_max = 5000\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    cnt += 1\n",
    "    for sent in doc.sents:\n",
    "        sent_ = []\n",
    "        tag_ = []\n",
    "\n",
    "        for word in sent.tokens:\n",
    "            tag_.append(word.pos)\n",
    "            sent_.append(word.text)\n",
    "\n",
    "        sentences.append(sent_)\n",
    "        tags.append(tag_)\n",
    "\n",
    "    if cnt > n_docs_max:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_tokens = {word for sent in sentences for word in sent}\n",
    "set_tokens.difference_update(special_tokens)\n",
    "list_tokens = special_tokens + list(set_tokens)\n",
    "\n",
    "set_tags = {tag for t in tags for tag in t}\n",
    "set_tags.difference_update(special_tags)\n",
    "list_tags = special_tags + list(set_tags)\n",
    "\n",
    "token_to_idx = dict(zip(list_tokens, np.arange(len(list_tokens))))\n",
    "tag_to_idx = dict(zip(list_tags, np.arange(len(list_tags))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_boundary = int(len(sentences) * 0.9)\n",
    "\n",
    "train_sentences = sentences[:train_test_boundary]\n",
    "train_tags = tags[:train_test_boundary]\n",
    "test_sentences = sentences[train_test_boundary:]\n",
    "test_tags = tags[train_test_boundary:]\n",
    "\n",
    "print(len(train_sentences), len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, tags, token_to_idx, tag_to_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "        sent_index = []\n",
    "        tags_index = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            sequence = []\n",
    "\n",
    "            for token in sent:\n",
    "                if token in self.token_to_idx:\n",
    "                    sequence.append(token_to_idx[token])\n",
    "                else:\n",
    "                    sequence.append(0)\n",
    "\n",
    "            sent_index.append(sequence)\n",
    "\n",
    "        for sent_tags in tags:\n",
    "            tgs = []\n",
    "\n",
    "            for tag in sent_tags:\n",
    "                tgs.append(tag_to_idx[tag])\n",
    "\n",
    "            tags_index.append(tgs)\n",
    "\n",
    "        self.sent_index = sent_index\n",
    "        self.tags_index = tags_index\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sent_index[idx]), torch.tensor(self.tags_index[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggingDataset(train_sentences, train_tags, token_to_idx, tag_to_idx)\n",
    "test_dataset = TaggingDataset(test_sentences, test_tags, token_to_idx, tag_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingCollator:\n",
    "    def __init__(self, pad_token_id, pad_tag_id):\n",
    "        self.pad_token_idx = pad_token_id\n",
    "        self.pad_tag_id = pad_tag_id\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        max_len = 0\n",
    "        for elem in batch:\n",
    "            if (len(elem[0]) > max_len):\n",
    "                max_len = len(elem[0])\n",
    "\n",
    "        new_sentences = []\n",
    "        new_tags = []\n",
    "\n",
    "        for elem in batch:\n",
    "            new_sentences.append(torch.nn.functional.pad(elem[0], (0, max_len - len(elem[0])), \"constant\", self.pad_token_idx))\n",
    "            new_tags.append(torch.nn.functional.pad(elem[1], (0, max_len - len(elem[1])), \"constant\", self.pad_tag_id))\n",
    "\n",
    "        return torch.stack(new_sentences), torch.stack(new_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=PaddingCollator(\n",
    "        pad_token_id=token_to_idx['<PAD>'],\n",
    "        pad_tag_id=tag_to_idx['<PAD>'],\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=PaddingCollator(\n",
    "        pad_token_id=token_to_idx['<PAD>'],\n",
    "        pad_tag_id=tag_to_idx['<PAD>'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    x_batch = x_batch.to(device)\n",
    "    y_batch = y_batch.to(device)\n",
    "\n",
    "    output = model(x_batch)\n",
    "    output = torch.transpose(output, 1, 2)\n",
    "    loss = loss_function(output, y_batch)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer, callback):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(\n",
    "            model, batch_of_x.to(device), batch_of_y.to(device), optimizer, loss_function)\n",
    "\n",
    "        if callback is not None:\n",
    "            callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss * len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            model,\n",
    "            dataset_loader,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr=0.001,\n",
    "            callback=None):\n",
    "    optima = optimizer(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    iterations = tqdm(range(count_of_epoch))\n",
    "\n",
    "    for it in iterations:\n",
    "        epoch_loss = train_epoch(\n",
    "            train_generator=dataset_loader, model=model,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optima,\n",
    "            callback=callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_of_train(dataset_loader,\n",
    "                     model,\n",
    "                     loss_function):\n",
    "    pred = []\n",
    "    real = []\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for it, (sentences, tags) in enumerate(dataset_loader):\n",
    "        sentences = sentences.to(device)\n",
    "        tags = tags.to(device)\n",
    "\n",
    "        output = model(sentences)\n",
    "        output = torch.transpose(output, 1, 2)\n",
    "\n",
    "        test_loss += loss_function(output, tags).cpu().item() * len(sentences)\n",
    "        total += len(sentences)\n",
    "\n",
    "        pred.extend(torch.argmax(output, dim=2).cpu().numpy().flatten().tolist())\n",
    "        real.extend(tags.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    test_loss /= total\n",
    "\n",
    "    return test_loss, pred, real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, dropout=0.0):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, x_batch):\n",
    "        embeddings = self.word_embeddings(x_batch)\n",
    "\n",
    "        d_n, (h_n, c_n) = self.lstm(embeddings)\n",
    "        return self.linear(d_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard training tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset_loader, loss_function, delimeter=100):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "        self.dataset_loader = dataset_loader\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "            test_loss, pred, real = quality_of_train(dataset_loader=self.dataset_loader,\n",
    "                                                     model=model, loss_function=self.loss_function)\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index = tag_to_idx['<PAD>'])\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParameterGrid({\n",
    "    'embedding_dim': [150, 290, 430, 600],\n",
    "    'hidden_dim': [150, 290, 430, 600],\n",
    "    'dropout': [0.0, 0.18, 0.36, 0.54],\n",
    "})\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for item in tqdm(grid):\n",
    "    print(str(item))\n",
    "\n",
    "    model = LSTMTagger(\n",
    "        embedding_dim=item['embedding_dim'],\n",
    "        hidden_dim=item['hidden_dim'],\n",
    "        vocab_size=len(token_to_idx),\n",
    "        tagset_size=len(tag_to_idx),\n",
    "        dropout=item['dropout']\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter('experiment/' + str(item))\n",
    "\n",
    "    model.float().to(device)\n",
    "\n",
    "    call = callback(writer, test_dataloader, loss_function, delimeter=10)\n",
    "\n",
    "    trainer(count_of_epoch=2,\n",
    "        dataset_loader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        lr=0.001,\n",
    "        callback=call)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
