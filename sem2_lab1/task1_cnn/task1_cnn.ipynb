{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip experiment.zip -d .\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./experiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 61\n",
    "torch.manual_seed(random_seed)\n",
    "batch_size_train = 64\n",
    "batch_size_test = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1307 and 0.3081 are just mean and std values for normalization\n",
    "\n",
    "train_dataset = FashionMNIST('.', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                                 torchvision.transforms.ToTensor(),\n",
    "                                 torchvision.transforms.Normalize(\n",
    "                                     (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "test_dataset = FashionMNIST('.', train=False, download=True,\n",
    "                            transform=torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(\n",
    "                                    (0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [45000, 15000])\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, x_batch, y_batch, optimizer, loss_function):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    output = model(x_batch.to(device))\n",
    "\n",
    "    loss = loss_function(output, y_batch.to(device))\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer, callback=None):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        batch_loss = train_on_batch(model, batch_of_x.to(device), batch_of_y.to(device), optimizer, loss_function)\n",
    "\n",
    "        if callback is not None:\n",
    "            callback(model, batch_loss)\n",
    "\n",
    "        epoch_loss += batch_loss * len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch,\n",
    "            batch_size,\n",
    "            dataset,\n",
    "            model,\n",
    "            loss_function,\n",
    "            optimizer,\n",
    "            lr=0.001,\n",
    "            callback=None):\n",
    "\n",
    "    optima = optimizer(model.parameters(), lr=lr)\n",
    "\n",
    "    iterations = tqdm(range(count_of_epoch), desc='epoch')\n",
    "    iterations.set_postfix({'train epoch loss': np.nan})\n",
    "    for it in iterations:\n",
    "        batch_generator = tqdm(\n",
    "            torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True),\n",
    "            leave=False, total=(len(dataset) // batch_size + (len(dataset) % batch_size > 0)))\n",
    "\n",
    "        epoch_loss = train_epoch(train_generator=batch_generator,\n",
    "                    model=model,\n",
    "                    loss_function=loss_function,\n",
    "                    optimizer=optima,\n",
    "                    callback=callback)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_of_train(batch_size,\n",
    "                     dataset,\n",
    "                     model,\n",
    "                     loss_function):\n",
    "\n",
    "    batch_generator = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                                  batch_size=batch_size)\n",
    "\n",
    "    pred = []\n",
    "    real = []\n",
    "    test_loss = 0\n",
    "\n",
    "    for it, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        output = model(x_batch)\n",
    "\n",
    "        test_loss += loss_function(output, y_batch).cpu().item() * len(x_batch)\n",
    "\n",
    "        pred.extend(torch.argmax(output, dim=-1).cpu().numpy().tolist())\n",
    "        real.extend(y_batch.cpu().numpy().tolist())\n",
    "\n",
    "    test_loss /= len(dataset)\n",
    "\n",
    "    return test_loss, pred, real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        for p in self.parameters():\n",
    "            return p.device\n",
    "\n",
    "    def __init__(self, n_layers=1, kernel_size=5, pooling=False, batch_norm=False, dropout=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_channels = 1\n",
    "        self.layers = torch.nn.Sequential()\n",
    "\n",
    "        for layer in range(n_layers):\n",
    "            self.layers.add_module('conv' + str(layer),\n",
    "                torch.nn.Conv2d(self.n_channels, self.n_channels * 4,\n",
    "                                kernel_size=kernel_size, padding=(kernel_size - 1) // 2))\n",
    "            self.n_channels *= 4\n",
    "\n",
    "            if batch_norm:\n",
    "                self.layers.add_module('bn' + str(layer), torch.nn.BatchNorm2d(self.n_channels))\n",
    "\n",
    "            self.layers.add_module('relu' + str(layer), torch.nn.ReLU())\n",
    "\n",
    "            if pooling:\n",
    "                self.layers.add_module('pool' + str(layer), torch.nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        self.layers.add_module('flatten', torch.nn.Flatten(start_dim=1))\n",
    "        self.layers.add_module('dropout1', torch.nn.Dropout(dropout))\n",
    "        self.layers.add_module('linear1',\n",
    "            torch.nn.Linear(int(self.n_channels * (28 // 2 ** n_layers if pooling else 28) ** 2), 10))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layers(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard training tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class callback():\n",
    "    def __init__(self, writer, dataset, loss_function, delimeter=100, batch_size=64):\n",
    "        self.step = 0\n",
    "        self.writer = writer\n",
    "        self.delimeter = delimeter\n",
    "        self.loss_function = loss_function\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def forward(self, model, loss):\n",
    "        self.step += 1\n",
    "        self.writer.add_scalar('LOSS/train', loss, self.step)\n",
    "\n",
    "        if self.step % self.delimeter == 0:\n",
    "\n",
    "            self.writer.add_graph(model, self.dataset[0][0].view(1, 1, 28, 28).to(model.device))\n",
    "\n",
    "            test_loss, pred, real = quality_of_train(batch_size=self.batch_size, dataset=self.dataset,\n",
    "                                                     model=model, loss_function=self.loss_function)\n",
    "            self.writer.add_scalar('LOSS/test', test_loss, self.step)\n",
    "\n",
    "    def __call__(self, model, loss):\n",
    "        return self.forward(model, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParameterGrid({\n",
    "    'layers': [2, 3],\n",
    "    'kernel_size': [3, 5, 7],\n",
    "    'bn': [True, False],\n",
    "    'pooling': [True, False],\n",
    "    'dropout': [0.0, 0.25, 0.5],\n",
    "})\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for item in tqdm(grid):\n",
    "    print(str(item))\n",
    "\n",
    "    model = CNN(\n",
    "        n_layers=item['layers'],\n",
    "        kernel_size=item['kernel_size'],\n",
    "        pooling=item['pooling'],\n",
    "        batch_norm=item['bn'],\n",
    "        dropout=item['dropout']\n",
    "    )\n",
    "\n",
    "    writer = SummaryWriter('experiment/' + str(item))\n",
    "\n",
    "    model.float().to(device)\n",
    "\n",
    "    call = callback(writer, test_dataset, loss_function, batch_size=batch_size_test, delimeter=10)\n",
    "\n",
    "    trainer(count_of_epoch=1,\n",
    "            batch_size=batch_size_train,\n",
    "            dataset=train_dataset,\n",
    "            model=model,\n",
    "            loss_function=loss_function,\n",
    "            optimizer=optimizer,\n",
    "            lr=0.001,\n",
    "            callback=call)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
