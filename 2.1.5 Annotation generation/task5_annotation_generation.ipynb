{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vokerlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from more_itertools import consume\n",
    "from pycocotools.coco import COCO\n",
    "from collections import Counter\n",
    "from threading import Thread\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 23224), started 11 days, 23:01:55 ago. (Use '!kill 23224' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-91010832ead4f63f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-91010832ead4f63f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !unzip experiment.zip -d .\n",
    "%reload_ext tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./experiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset & general parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dataset, resize images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачивание dataset'а будет очень долгим, поэтому код ниже комменчен. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to load datasets\n",
    "# !wget http://images.cocodataset.org/zips/train2014.zip\n",
    "# !wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "\n",
    "# !unzip train2014.zip\n",
    "# !unzip annotations_trainval2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm train2014.zip\n",
    "# !rm annotations_trainval2014.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для единообразия и экономии времени сожмём картинки до более худшего качества.\n",
    "\n",
    "Создадим на каждую картинку отдельный thread, который сожмёт картинку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: /bin/rm: Argument list too long\n"
     ]
    }
   ],
   "source": [
    "!rm -rf resized_images/*\n",
    "!mkdir -p resized_images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82783/82783 [01:14<00:00, 1106.07it/s]\n",
      "100%|██████████| 82783/82783 [00:00<00:00, 1114822.58it/s]\n"
     ]
    }
   ],
   "source": [
    "img_path_to_resize = 'train2014'\n",
    "images_to_resize = os.listdir(img_path_to_resize)\n",
    "\n",
    "# Function just for future tests & training, not for resizing\n",
    "def load_image(image_path, transform=None, basewidth=128):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize([basewidth, basewidth], Image.LANCZOS)\n",
    "\n",
    "    if transform is not None:\n",
    "        img = transform(img).unsqueeze(0)\n",
    "\n",
    "    return img\n",
    "\n",
    "def resize_image(img_name, basewidth=128):\n",
    "    img = Image.open(img_path_to_resize + '/' + img_name)\n",
    "    img = img.resize((basewidth, basewidth), Image.LANCZOS)\n",
    "    img.save('resized_images/' + img_name)\n",
    "\n",
    "img_load_threads = []\n",
    "for img_name in tqdm(images_to_resize):\n",
    "    thread = Thread(target=resize_image, kwargs={'img_name': img_name})\n",
    "    thread.start()\n",
    "    img_load_threads.append(thread)\n",
    "\n",
    "consume(map(lambda t: t.join(), tqdm(img_load_threads)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling vocabulary using extracted tokens from annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На будущее создадим словарь \"первичного\" embedding'а максимально простым способом: присваивая новым словая последовательные индексы, начиная с нуля."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обозначения старта и конца предложения добавим сразу в словарь `<start>` и `<end>`, а также `<pad>` и `<unk>` для выравниваний и неизвестых слов соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.n_idx = 0\n",
    "\n",
    "        self.add_word('<pad>')\n",
    "        self.add_word('<start>')\n",
    "        self.add_word('<end>')\n",
    "        self.add_word('<unk>')\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.n_idx\n",
    "            self.idx2word[self.n_idx] = word\n",
    "            self.n_idx += 1\n",
    "\n",
    "    def restore_caption(self, caption_idxs):\n",
    "        caption = []\n",
    "\n",
    "        for word_idx in caption_idxs:\n",
    "            word = self.idx2word[word_idx]\n",
    "            if (word != '<pad>') and (word != '<start>') and \\\n",
    "               (word != '<unk>') and (word != '<end>'):\n",
    "                caption.append(word)\n",
    "\n",
    "            if word == '<end>':\n",
    "                break\n",
    "\n",
    "        return caption\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним словарь словами из датасета COCO, отсекая наименее часто встречающиеся слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(json, freq_threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter() # map for counting unique words\n",
    "    ids = coco.anns.keys()\n",
    "\n",
    "    for id in ids:\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # If the word frequency is less than 'freq_threshold', then the word is discarded.\n",
    "    # So only the most popular words participate in vocabulary -> models.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= freq_threshold]\n",
    "\n",
    "    vocab = Vocabulary()\n",
    "\n",
    "    for word in words:\n",
    "        vocab.add_word(word)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "json_path = 'annotations/captions_train2014.json'\n",
    "vocab = build_vocab(json=json_path, freq_threshold=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of torch-dataloader from annotations and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим обёртку над `torch.utils.data.Dataset`'ом, чтобы абстрагироваться от формата данных, с помощью которых происходит обучение.\n",
    "\n",
    "Наиболее гланый метод в `CocoDataset` — `__getitem__`, который нужен для функциии `collate_fn` для создания batch'а image-caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, root, json, vocab, transform):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "\n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    # Generally, for collate_fn() function (see below).\n",
    "    # Return tensors of image and its caption.\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\n",
    "        Args:\n",
    "            index: index in image-caption dataset\n",
    "        \"\"\"\n",
    "        # Get raw image & caption for input index\n",
    "        annotation_id = self.ids[index]\n",
    "        caption = self.coco.anns[annotation_id]['caption']\n",
    "        image_id = self.coco.anns[annotation_id]['image_id']\n",
    "\n",
    "        path = self.coco.loadImgs(image_id)[0]['file_name']\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "\n",
    "        # Convert caption to vector (using vocabulary)\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "\n",
    "        return self.transform(image), torch.Tensor(caption)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, имея класс датасета, можем себе позволить организовать генерацию `DataLoader`.\n",
    "\n",
    "Для этого следует перегрузить дефолтную реализацию `collate_fn`, чтобы обеспечить корректное создание batch'а: caption'ы должны склеиться с учётом padding'а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "\n",
    "    We should build custom collate_fn rather than using default collate_fn,\n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption).\n",
    "            - image: torch tensor of shape (3, 128, 128).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 128, 128).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list indicating valid length for each caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor) within batch dimension\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor) within batch dimension\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "\n",
    "    # Filling zero-tensor 2D with captions\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "\n",
    "    return images, targets, lengths\n",
    "\n",
    "def generate_data_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom COCO dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       json=json,\n",
    "                       vocab=vocab,\n",
    "                       transform=transform)\n",
    "\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 128, 128).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. len(lengths) == batch_size\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве модели encoder будем использовать предобученую модель `resnet152` без последнего слоя.\n",
    "\n",
    "В качестве модели decoder возьмём `LSTM` (вместе с `Embedding` слоем)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(torch.nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        resnet = torchvision.models.resnet152(weights=torchvision.models.ResNet152_Weights.DEFAULT)\n",
    "        modules = list(resnet.children())[:-1] # remove the last layer\n",
    "\n",
    "        self.resnet = torch.nn.Sequential(*modules)\n",
    "        self.linear = torch.nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = torch.nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        return self.bn(self.linear(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_sent_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = torch.nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size) # interpretation: probabilities for each word in vocabulary\n",
    "        self.max_sent_length = max_sent_length\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        hiddens, _ = self.lstm(packed)\n",
    "\n",
    "        return self.linear(hiddens[0])\n",
    "\n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_idxs = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "\n",
    "        for _ in range(self.max_sent_length):\n",
    "            # Restore word for batch                             # inputs: (batch_size, 1, embed_size)\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "\n",
    "            # Save predicted words for batcn\n",
    "            sampled_idxs.append(predicted)\n",
    "\n",
    "            # Get new input for next prediction\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "\n",
    "        return torch.stack(sampled_idxs, 1)                      # sampled_idxs: (batch_size, max_sent_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'resized_images/'\n",
    "caption_path = json_path\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 6\n",
    "log_step = 10\n",
    "\n",
    "writer = SummaryWriter(log_dir='experiment/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# For image transformation -> tensor\n",
    "transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.RandomCrop(64),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                 (0.229, 0.224, 0.225))]) # officially used values by pytorch for image normalization\n",
    "\n",
    "# Build data loader\n",
    "data_loader = generate_data_loader(image_dir, caption_path, vocab,\n",
    "                                   transform, batch_size,\n",
    "                                   shuffle=True, num_workers=num_workers)\n",
    "\n",
    "# Build the models\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "total_step = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем картинку, за которой будем поэтапно следить во время обучения (точнее, за её аннотациями)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "test_img_name = 'resized_images/' + os.listdir('resized_images/')[42]\n",
    "test_image = load_image(test_img_name, transform)\n",
    "test_image_tensor = test_image.to(device)\n",
    "writer.add_image(f'Image {test_img_name}', load_image(test_img_name, transform=simple_transform), dataformats='NCHW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запустим само обучение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs), leave=False):\n",
    "    for i, (images, captions, lengths) in enumerate(tqdm(data_loader, leave=False)):\n",
    "\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = torch.nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info\n",
    "        if i % log_step == 0:\n",
    "            writer.add_scalar('LOSS/train', loss.item(), i + total_step * epoch)\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                feature = encoder(test_image_tensor)\n",
    "                sampled_idxs = decoder.sample(feature)\n",
    "                sampled_idxs = sampled_idxs[0].cpu().numpy() # [0] because of batch with batcn_size=1\n",
    "\n",
    "                sampled_caption = vocab.restore_caption(sampled_idxs)\n",
    "\n",
    "                sentence = ' '.join(sampled_caption)\n",
    "                writer.add_text('Captions', sentence, i + total_step * epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По итогу всё получилось примерно так, как и ожидалось. На картинке стоит человек около около прилавок с фруктами (апельсины и яблоки).\n",
    "\n",
    "Модель на последних эпохах утверждает, что на картинке стоит человек в костюме напротив каких-либо объектов, что максимально похоже на правду, с учётом того, что на сжатой картинке детализация прилавок сильно пострадала."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
